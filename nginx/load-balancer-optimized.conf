# =============================================================================
# OPTIMIZED Load Balancer Configuration for Arena Backend
# Target: 16 vCPU, 64 GB RAM with 6 backend containers
# =============================================================================

# -----------------------------------------------------------------------------
# UPSTREAM: Django Backend (General Requests)
# -----------------------------------------------------------------------------
# Using least_conn for better distribution with varying request times
upstream django_backend {
    least_conn;

    # Keepalive connections - reduces TCP overhead
    # Formula: (workers_per_container × containers) × 2 = (4 × 6) × 2 = 48
    # Round up to 64 for headroom
    keepalive 64;
    keepalive_timeout 75s;
    keepalive_requests 10000;

    # Backend servers with health-aware settings
    # - max_fails=3: Mark down after 3 consecutive failures
    # - fail_timeout=30s: How long to wait before retrying failed server
    # - max_conns=100: Limit concurrent connections (prevents overload)
    #   Formula: (threads × workers) / 1.5 = (4 × 4) / 1.5 ≈ 10-12, but allow burst
    # - slow_start=30s: Gradually ramp up traffic to recovered servers

    server web-1:8000 max_fails=3 fail_timeout=30s max_conns=100 weight=1;
    server web-2:8000 max_fails=3 fail_timeout=30s max_conns=100 weight=1;
    server web-3:8000 max_fails=3 fail_timeout=30s max_conns=100 weight=1;
    server web-4:8000 max_fails=3 fail_timeout=30s max_conns=100 weight=1;
    server web-5:8000 max_fails=3 fail_timeout=30s max_conns=100 weight=1;
    server web-6:8000 max_fails=3 fail_timeout=30s max_conns=100 weight=1;
}

# -----------------------------------------------------------------------------
# UPSTREAM: Streaming Endpoints (SSE/Long-polling)
# -----------------------------------------------------------------------------
# Dedicated upstream for streaming to prevent blocking regular requests
upstream django_streaming {
    least_conn;

    # More keepalive for streaming (long-lived connections)
    keepalive 128;
    keepalive_timeout 600s;
    keepalive_requests 100;

    # Higher max_conns for streaming (these are long-lived)
    server web-1:8000 max_fails=3 fail_timeout=30s max_conns=150 weight=1;
    server web-2:8000 max_fails=3 fail_timeout=30s max_conns=150 weight=1;
    server web-3:8000 max_fails=3 fail_timeout=30s max_conns=150 weight=1;
    server web-4:8000 max_fails=3 fail_timeout=30s max_conns=150 weight=1;
    server web-5:8000 max_fails=3 fail_timeout=30s max_conns=150 weight=1;
    server web-6:8000 max_fails=3 fail_timeout=30s max_conns=150 weight=1;
}

# -----------------------------------------------------------------------------
# RATE LIMITING ZONES
# -----------------------------------------------------------------------------
# Tuned for 6 containers with 16 concurrent handlers each = 96 total capacity
# Allow ~30% of capacity per IP to prevent single IP monopolizing

# General API: 30 requests/second per IP (burst allows short spikes)
limit_req_zone $binary_remote_addr zone=general_limit:20m rate=30r/s;

# Streaming: 5 requests/second per IP (long-lived, fewer needed)
limit_req_zone $binary_remote_addr zone=streaming_limit:20m rate=5r/s;

# Auth endpoints: 3 requests/second per IP (prevent brute force)
limit_req_zone $binary_remote_addr zone=auth_limit:10m rate=3r/s;

# Connection limit per IP (total concurrent connections)
limit_conn_zone $binary_remote_addr zone=conn_limit:20m;

# -----------------------------------------------------------------------------
# PROXY CACHE
# -----------------------------------------------------------------------------
proxy_cache_path /var/cache/nginx
    levels=1:2
    keys_zone=api_cache:20m
    max_size=2g
    inactive=60m
    use_temp_path=off;

# -----------------------------------------------------------------------------
# PROXY SETTINGS (Applied globally)
# -----------------------------------------------------------------------------
# NOTE: Buffer settings are configured per-location in the vhost template
# to avoid conflicts with nginx.conf defaults

# Connection reuse
proxy_http_version 1.1;
proxy_set_header Connection "";
